# ğŸ§  MLP from Scratch: AdamW + Cosine Scheduler

Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾ÑĞµÑ‚Ğ¸ **Ñ Ğ½ÑƒĞ»Ñ Ğ½Ğ° NumPy**

**Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ°:** ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ CIFAR-10 Ğ½Ğ° ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ°Ñ… Ğ¸Ğ· Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ ViT.

```
Frozen ViT â†’ Embeddings â†’ MLP (Ğ½Ğ°ÑˆĞ° NumPy-Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ) â†’ 10 ĞºĞ»Ğ°ÑÑĞ¾Ğ²
```

---

## ğŸ¯ Ğ§Ñ‚Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ (NumPy)

| ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚ | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ |
|----------|----------|
| `Linear` | ĞŸĞ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ Ñ He-Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ |
| `ReLU`, `GELU` | Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ |
| `Dropout` | Ğ ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ |
| `CrossEntropyLoss` | Softmax + NLL (numerically stable) |
| `AdamW` | Adam Ñ decoupled weight decay |
| `CosineScheduler` | Cosine annealing Ñ warmup |
| `Backpropagation` | Chain rule Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑĞµ ÑĞ»Ğ¾Ğ¸ |

---

## ğŸ“ Ğ¡Ñ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° (Ğ°ĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ°Ñ)

```
mlp_from_scratch/
â”œâ”€â”€ config.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_exp_1.py           # NumPy Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (from scratch)
â”œâ”€â”€ run_exp_2.py           # PyTorch baseline
â”œâ”€â”€ grad_check.py          # Ğ§Ğ¸ÑĞ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚-Ñ‡ĞµĞº (finite differences)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ embeddings.py      # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ğ¸Ğ· ViT + ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
â”‚
â”œâ”€â”€ numpy_impl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ layers.py          # Linear, Dropout, base Layer
â”‚   â”œâ”€â”€ activations.py     # ReLU, GELU, Softmax
â”‚   â”œâ”€â”€ losses.py          # CrossEntropyLoss (+ compute_accuracy)
â”‚   â”œâ”€â”€ model.py           # MLP, Sequential
â”‚   â”œâ”€â”€ optimizers.py      # SGD, AdamW
â”‚   â””â”€â”€ schedulers.py      # CosineScheduler, StepScheduler
â”‚
â””â”€â”€ torch_impl/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ model.py           # TorchMLP
    â””â”€â”€ trainer.py         # Training loop (+ create_dataloaders)
```

---

## ğŸš€ Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ€Ñ‚

```bash
pip install -r requirements.txt
wandb login

python run_exp_1.py
python run_exp_2.py
```

---

## ğŸ§ª Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚-Ñ‡ĞµĞº (Ğ²Ğ°Ğ¶Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ)

Ğ§Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ backprop Ñ‡ĞµÑ€ĞµĞ· `MLP + CrossEntropyLoss`:

```bash
python grad_check.py
```

---

## ğŸ§ª Smoke test (forward/backward)

```bash
python - <<'PY'
from numpy_impl import MLP, CrossEntropyLoss
import numpy as np

np.random.seed(0)

model = MLP(input_dim=10, hidden_dims=[32], output_dim=3)
criterion = CrossEntropyLoss()

X = np.random.randn(4, 10)
y = np.array([0, 1, 2, 1])

logits = model.forward(X)
loss = criterion(logits, y)
print(f"Loss: {loss:.4f}")
print("âœ… Forward OK")

dout = criterion.backward()
model.backward(dout)
print("âœ… Backward OK")
PY
```

---

## âš™ï¸ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ

Ğ¡Ğ¼Ğ¾Ñ‚Ñ€Ğ¸ `config.py`. Ğ’Ğ°Ğ¶Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ Ğ´Ğ»Ñ ÑĞ¾Ğ³Ğ»Ğ°ÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ñ warmup Ğ¼ĞµĞ¶Ğ´Ñƒ NumPy Ğ¸ PyTorch:

- `warmup_start_factor`: ÑÑ‚Ğ°Ñ€Ñ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ¼Ğ½Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒ learning rate Ğ½Ğ° warmup (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ `0.01` Ğ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚ â€œĞ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ñ 1% Ğ¾Ñ‚ lrâ€).

---
```

---

### config.py
```py
from dataclasses import dataclass, field
from typing import List


@dataclass
class Config:
    # Data
    data_dir: str = "./data"
    embedding_model: str = "vit_small_patch16_224"
    num_classes: int = 10

    # Model
    hidden_dims: List[int] = field(default_factory=lambda: [512, 256])
    dropout: float = 0.0

    # Training
    epochs: int = 30
    batch_size: int = 128
    lr: float = 3e-4
    weight_decay: float = 0.05

    # LR schedule
    warmup_epochs: int = 3
    warmup_start_factor: float = 0.01  # start lr = lr * warmup_start_factor
    lr_min: float = 1e-6

    # Adam
    beta1: float = 0.9
    beta2: float = 0.999
    eps: float = 1e-8

    # Wandb
    project: str = "mlp-from-scratch"

    # Misc
    seed: int = 42


def get_config(**kwargs) -> Config:
    """Create config with optional overrides."""
    return Config(**kwargs)
```

---

### numpy_impl/schedulers.py
```py
import numpy as np
from .optimizers import Optimizer


class Scheduler:
    """Base class for learning rate schedulers."""

    def __init__(self, optimizer: Optimizer):
        self.optimizer = optimizer
        self.base_lr = optimizer.lr
        # Make steps 0-based like many schedulers:
        # first call to step() sets current_step = 0
        self.current_step = -1

    def step(self):
        raise NotImplementedError

    def get_lr(self) -> float:
        return self.optimizer.lr


class CosineScheduler(Scheduler):
    """
    Cosine Annealing Learning Rate Scheduler with optional warmup.

    Warmup is linear in *factor space*:
        lr = lr_max * (start_factor + (1 - start_factor) * progress)

    where progress goes from 0 to 1 during warmup.

    After warmup:
        lr = lr_min + 0.5 * (lr_max - lr_min) * (1 + cos(pi * progress))
    """

    def __init__(
        self,
        optimizer: Optimizer,
        total_steps: int,
        warmup_steps: int = 0,
        lr_min: float = 0.0,
        warmup_start_factor: float = 0.0,
    ):
        super().__init__(optimizer)

        if total_steps <= 0:
            raise ValueError(f"total_steps must be > 0, got {total_steps}")

        if warmup_steps < 0:
            raise ValueError(f"warmup_steps must be >= 0, got {warmup_steps}")

        if not (0.0 <= warmup_start_factor <= 1.0):
            raise ValueError(
                f"warmup_start_factor must be in [0, 1], got {warmup_start_factor}"
            )

        self.total_steps = int(total_steps)
        self.warmup_steps = int(warmup_steps)
        self.lr_min = float(lr_min)
        self.warmup_start_factor = float(warmup_start_factor)

        self.lr_max = optimizer.lr

    def step(self) -> float:
        """
        Update learning rate and return current value.
        This scheduler is step-based (call once per optimizer step).
        """
        self.current_step += 1
        s = self.current_step  # 0-based step index

        # If someone calls step() beyond total_steps, clamp to final lr
        if s >= self.total_steps:
            self.optimizer.set_lr(self.lr_min)
            return self.lr_min

        # --------------------
        # Warmup
        # --------------------
        if self.warmup_steps > 0 and s < self.warmup_steps:
            if self.warmup_steps == 1:
                progress = 1.0
            else:
                # s = 0 ... warmup_steps-1 => progress 0 ... 1
                progress = s / (self.warmup_steps - 1)

            factor = self.warmup_start_factor + (1.0 - self.warmup_start_factor) * progress
            lr = self.lr_max * factor
            self.optimizer.set_lr(lr)
            return lr

        # --------------------
        # Cosine
        # --------------------
        cosine_steps = self.total_steps - self.warmup_steps
        t = s - self.warmup_steps  # 0-based index in cosine segment

        if cosine_steps <= 1:
            lr = self.lr_min
        else:
            # t = 0 ... cosine_steps-1 => progress 0 ... 1
            progress = t / (cosine_steps - 1)
            lr = self.lr_min + 0.5 * (self.lr_max - self.lr_min) * (1 + np.cos(np.pi * progress))

        self.optimizer.set_lr(lr)
        return lr

    def state_dict(self) -> dict:
        return {"current_step": self.current_step}

    def load_state_dict(self, state: dict):
        self.current_step = state["current_step"]


class StepScheduler(Scheduler):
    """Step decay: multiply lr by gamma every step_size steps."""

    def __init__(self, optimizer: Optimizer, step_size: int, gamma: float = 0.1):
        super().__init__(optimizer)
        self.step_size = int(step_size)
        self.gamma = float(gamma)

    def step(self) -> float:
        self.current_step += 1
        s = self.current_step

        n_decays = (s + 1) // self.step_size  # decay after step_size updates
        lr = self.base_lr * (self.gamma ** n_decays)

        self.optimizer.set_lr(lr)
        return lr
