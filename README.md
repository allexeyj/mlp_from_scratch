# README.md

```markdown
# üß† MLP from Scratch: AdamW + Cosine Scheduler

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ **—Å –Ω—É–ª—è –Ω–∞ NumPy** –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ–¥ –∫–∞–ø–æ—Ç–æ–º PyTorch.

**–ó–∞–¥–∞—á–∞:** –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è CIFAR-10 –Ω–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö –∏–∑ –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω–æ–≥–æ ViT.

```
Frozen ViT ‚Üí Embeddings ‚Üí MLP (–Ω–∞—à–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è) ‚Üí 10 –∫–ª–∞—Å—Å–æ–≤
```

---

## üéØ –ß—Ç–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –≤—Ä—É—á–Ω—É—é

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –û–ø–∏—Å–∞–Ω–∏–µ |
|-----------|----------|
| `Linear` | –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π —Å He-–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π |
| `ReLU`, `GELU` | –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ |
| `Dropout` | –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è |
| `CrossEntropyLoss` | Softmax + NLL (numerically stable) |
| `AdamW` | Adam —Å decoupled weight decay |
| `CosineScheduler` | Cosine annealing —Å warmup |
| `Backpropagation` | Chain rule —á–µ—Ä–µ–∑ –≤—Å–µ —Å–ª–æ–∏ |

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
mlp_from_scratch/
‚îú‚îÄ‚îÄ config.py              # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ run_exp_1.py           # üîß NumPy —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
‚îú‚îÄ‚îÄ run_exp_2.py           # üî• PyTorch baseline
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py      # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ ViT
‚îÇ   ‚îî‚îÄ‚îÄ batching.py        # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∏
‚îÇ
‚îú‚îÄ‚îÄ numpy_impl/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ layers.py          # Linear, Dropout
‚îÇ   ‚îú‚îÄ‚îÄ activations.py     # ReLU, GELU, Softmax
‚îÇ   ‚îú‚îÄ‚îÄ losses.py          # CrossEntropyLoss
‚îÇ   ‚îú‚îÄ‚îÄ model.py           # MLP, Sequential
‚îÇ   ‚îú‚îÄ‚îÄ optimizers.py      # SGD, AdamW
‚îÇ   ‚îî‚îÄ‚îÄ schedulers.py      # CosineScheduler
‚îÇ
‚îú‚îÄ‚îÄ torch_impl/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ model.py           # TorchMLP
‚îÇ   ‚îî‚îÄ‚îÄ trainer.py         # Training loop
‚îÇ
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îî‚îÄ‚îÄ reproducibility.py # Seed management
```

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

```bash
# –ö–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
git clone https://github.com/your-username/mlp-from-scratch.git
cd mlp-from-scratch

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install -r requirements.txt

# –ó–∞–ª–æ–≥–∏–Ω–∏—Ç—å—Å—è –≤ wandb
wandb login

# –ó–∞–ø—É—Å—Ç–∏—Ç—å NumPy —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
python run_exp_1.py

# –ó–∞–ø—É—Å—Ç–∏—Ç—å PyTorch —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç
python run_exp_2.py
```

---

## üìä –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

| –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç | Test Accuracy | –í—Ä–µ–º—è (GPU) |
|-------------|---------------|-------------|
| NumPy (exp1) | ~97.5% | ~2 –º–∏–Ω |
| PyTorch (exp2) | ~97.5% | ~1 –º–∏–Ω |

–ú–µ—Ç—Ä–∏–∫–∏ –ª–æ–≥–∏—Ä—É—é—Ç—Å—è –≤ [Weights & Biases](https://wandb.ai).

---

## üìê –¢–µ–æ—Ä–∏—è

### AdamW (Adam with Decoupled Weight Decay)

–ö–ª—é—á–µ–≤–æ–µ –æ—Ç–ª–∏—á–∏–µ –æ—Ç Adam + L2:

```
‚ùå Adam + L2:    grad = grad + Œª¬∑Œ∏,  –∑–∞—Ç–µ–º Adam update
‚úÖ AdamW:        Adam update,        –∑–∞—Ç–µ–º Œ∏ -= lr¬∑Œª¬∑Œ∏
```

**–ü–æ—á–µ–º—É —ç—Ç–æ –≤–∞–∂–Ω–æ?** Adam –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∞–¥–∞–ø—Ç–∏–≤–Ω–æ (–¥–µ–ª–∏—Ç –Ω–∞ ‚àöv). –ü—Ä–∏ L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è —Ç–æ–∂–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è, —á—Ç–æ –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ.

**–§–æ—Ä–º—É–ª—ã:**
```
m_t = Œ≤‚ÇÅ¬∑m_{t-1} + (1-Œ≤‚ÇÅ)¬∑g_t           # –ü–µ—Ä–≤—ã–π –º–æ–º–µ–Ω—Ç (momentum)
v_t = Œ≤‚ÇÇ¬∑v_{t-1} + (1-Œ≤‚ÇÇ)¬∑g_t¬≤          # –í—Ç–æ—Ä–æ–π –º–æ–º–µ–Ω—Ç (adaptive lr)
mÃÇ_t = m_t / (1-Œ≤‚ÇÅ·µó)                     # Bias correction
vÃÇ_t = v_t / (1-Œ≤‚ÇÇ·µó)                     # Bias correction
Œ∏_t = Œ∏_{t-1} - lr¬∑(mÃÇ_t/‚àö(vÃÇ_t+Œµ) + Œª¬∑Œ∏_{t-1})  # Decoupled!
```

### Cosine Scheduler

```
lr(t) = lr_min + 0.5¬∑(lr_max - lr_min)¬∑(1 + cos(œÄ¬∑t/T))
```

```
lr_max ‚îÄ‚îê
        ‚îÇ‚ï≤
        ‚îÇ ‚ï≤
        ‚îÇ  ‚ï≤      ‚Üê –ü–ª–∞–≤–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ
        ‚îÇ   ‚ï≤
        ‚îÇ    ‚ï≤
lr_min ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ï≤‚îÄ‚îÄ‚îÄ
        0    T_max
```

–° warmup: –ø–µ—Ä–≤—ã–µ N —à–∞–≥–æ–≤ lr —Ä–∞—Å—Ç—ë—Ç –ª–∏–Ω–µ–π–Ω–æ –æ—Ç 0 –¥–æ lr_max.

### Chain Rule (Backpropagation)

–ö–∞–∂–¥—ã–π —Å–ª–æ–π –≤ `backward()` –ø—Ä–∏–º–µ–Ω—è–µ—Ç chain rule:

```
dL/dx = dL/dy ¬∑ dy/dx
        ‚Üë        ‚Üë
        ‚îÇ        ‚îî‚îÄ‚îÄ –ª–æ–∫–∞–ª—å–Ω—ã–π –≥—Ä–∞–¥–∏–µ–Ω—Ç (–≤—ã—á–∏—Å–ª—è–µ–º)
        ‚îî‚îÄ‚îÄ –≥—Ä–∞–¥–∏–µ–Ω—Ç "—Å–≤–µ—Ä—Ö—É" (–ø–æ–ª—É—á–∞–µ–º –∫–∞–∫ dout)
```

**–ü—Ä–∏–º–µ—Ä –¥–ª—è Linear (y = x @ W + b):**
```python
def backward(self, dout):
    # dout = dL/dy (–ø—Ä–∏—à—ë–ª —Å–≤–µ—Ä—Ö—É)
    
    dW = x.T @ dout      # dL/dW = dL/dy ¬∑ dy/dW
    db = dout.sum(0)     # dL/db = dL/dy ¬∑ dy/db  
    dx = dout @ W.T      # dL/dx = dL/dy ¬∑ dy/dx (–ø–µ—Ä–µ–¥–∞—ë–º –¥–∞–ª—å—à–µ)
    
    return dx
```

**–ü—Ä–∏–º–µ—Ä –¥–ª—è ReLU (y = max(0, x)):**
```python
def backward(self, dout):
    # dy/dx = 1 if x > 0 else 0
    return dout * (x > 0)
```

---

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

–†–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ `config.py`:

```python
@dataclass
class Config:
    # Model
    hidden_dims: List[int] = [512, 256]
    dropout: float = 0.0
    
    # Training
    epochs: int = 30
    batch_size: int = 128
    lr: float = 3e-4
    weight_decay: float = 0.05
    warmup_epochs: int = 3
    
    # Adam
    beta1: float = 0.9
    beta2: float = 0.999
```

---

## üî¨ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã

### Exp 1: NumPy (from scratch)

–í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –≤—Ä—É—á–Ω—É—é. PyTorch –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è **—Ç–æ–ª—å–∫–æ** –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.

```python
# –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—à –∫–æ–¥:
model = MLP(input_dim=384, hidden_dims=[512, 256], output_dim=10)
optimizer = AdamW(model.get_params(), lr=3e-4, weight_decay=0.05)
scheduler = CosineScheduler(optimizer, total_steps=1000, warmup_steps=100)

for epoch in range(epochs):
    for X_batch, y_batch in batches:
        logits = model.forward(X_batch)
        loss = criterion(logits, y_batch)
        dout = criterion.backward()
        model.backward(dout)
        optimizer.step(model.get_grads())
        scheduler.step()
```

### Exp 2: PyTorch (baseline)

–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π PyTorch –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:

```python
model = TorchMLP(input_dim=384, hidden_dims=[512, 256], output_dim=10)
optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.05)
scheduler = CosineAnnealingLR(optimizer, T_max=1000)

for epoch in range(epochs):
    for X_batch, y_batch in loader:
        logits = model(X_batch)
        loss = F.cross_entropy(logits, y_batch)
        loss.backward()
        optimizer.step()
        scheduler.step()
```

---

## üìà –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ W&B

–ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –≤ W&B –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã:

- Loss curves (train/test)
- Accuracy curves (train/test)  
- Learning rate schedule
- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ NumPy vs PyTorch

---

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ forward/backward —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
python -c "
from numpy_impl import MLP, CrossEntropyLoss
import numpy as np

# Smoke test
model = MLP(input_dim=10, hidden_dims=[32], output_dim=3)
X = np.random.randn(4, 10)
y = np.array([0, 1, 2, 1])

logits = model.forward(X)
loss = CrossEntropyLoss()(logits, y)
print(f'Loss: {loss:.4f}')
print('‚úÖ Forward OK')

dout = CrossEntropyLoss().backward()
model.backward(dout)
print('‚úÖ Backward OK')
"
```

---

## üìö –°—Å—ã–ª–∫–∏

- [AdamW Paper](https://arxiv.org/abs/1711.05101) ‚Äî Decoupled Weight Decay Regularization
- [Cosine Annealing Paper](https://arxiv.org/abs/1608.03983) ‚Äî SGDR: Stochastic Gradient Descent with Warm Restarts
- [ViT Paper](https://arxiv.org/abs/2010.11929) ‚Äî An Image is Worth 16x16 Words
- [Backpropagation Explained](http://cs231n.stanford.edu/slides/2024/lecture_4.pdf) ‚Äî CS231n Lecture

---

## üìù License

MIT

---

<p align="center">
  <b>–°–¥–µ–ª–∞–Ω–æ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ç–æ–≥–æ, —á—Ç–æ —Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è –∑–∞ <code>loss.backward()</code></b>
</p>
```

---

–ì–æ—Ç–æ–≤–æ! –•–æ—á–µ—à—å —á—Ç–æ-—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å/–∏–∑–º–µ–Ω–∏—Ç—å ‚Äî badges, —Å–µ–∫—Ü–∏—é contributing, –±–æ–ª—å—à–µ —Ç–µ–æ—Ä–∏–∏?
